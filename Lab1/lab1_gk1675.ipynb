{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE-GY 9163 Lab1 Guandong Kou (gk1675)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description of this lab is re-organized in this [Google doc](https://docs.google.com/document/d/1wp9a-ns_xwh2_x5FIJFrzo0JQZdOY9Dwtg9LXW8pYVw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename 'part10' to 'validation_set' at `lemm_stop` directory\n",
    "```!mv part10 validation_set```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import text\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "import re\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data_path = 'data/lemm_stop/part*/'\n",
    "spam_files = glob(data_path + 'spms*[0-9]*.txt')\n",
    "ham_files = glob(data_path + '*[0-9]*msg[0-9]*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_spam, n_ham = len(spam_files), len(ham_files)\n",
    "n_total = n_spam + n_ham\n",
    "p_spam, p_ham = n_spam / n_total, n_ham / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counter(files):\n",
    "    # get word counter from files\n",
    "    remove_set = set(punctuation) | set(text.ENGLISH_STOP_WORDS)\n",
    "    w_counter = Counter()\n",
    "    for file in files:\n",
    "        word_set = set()\n",
    "        with open(file) as f:\n",
    "            for line in f.readlines():\n",
    "                for word in word_tokenize(line):\n",
    "                    if word not in remove_set and re.match('[a-zA-Z]{2,}', word):\n",
    "                        word_set.add(word.lower())\n",
    "        for word in word_set:\n",
    "            w_counter[word] += 1\n",
    "    return w_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_word_counter = get_word_counter(ham_files)\n",
    "spam_word_counter = get_word_counter(spam_files)\n",
    "total_word_counter = ham_word_counter + spam_word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(p):\n",
    "    return -(p * log2(p) + (1-p) * log2(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_word(word):\n",
    "    return total_word_counter[word] / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_H_C_X(word):\n",
    "    # Laplacian smoothing is applied here\n",
    "    p_word = total_word_counter[word] / n_total\n",
    "    p_ham_cond_word = (ham_word_counter[word] + 1) / (total_word_counter[word] + 2) #\n",
    "    p_ham_cond_not_word = (n_ham - ham_word_counter[word] + 1) / (n_total - total_word_counter[word] + 2) #\n",
    "    return p_word * get_entropy(p_ham_cond_word) + (1-p_word) * get_entropy(p_ham_cond_not_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_C = get_entropy(p_spam)\n",
    "def get_IG(word):\n",
    "    H_C_X = get_H_C_X(word)\n",
    "    return H_C - H_C_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.2014815280269308, 'language'),\n",
       " (0.1670505008347254, 'remove'),\n",
       " (0.16449082041657143, 'free'),\n",
       " (0.14515234460220117, 'linguistic'),\n",
       " (0.14253899233783307, 'university'),\n",
       " (0.11749366725618493, 'money'),\n",
       " (0.09944312815537659, 'click'),\n",
       " (0.09135660326353257, 'market'),\n",
       " (0.08584499224521047, 'business'),\n",
       " (0.0804619418318766, 'today')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 words with largest information gain \n",
    "nlargest(10, [(get_IG(w), w) for w in total_word_counter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
