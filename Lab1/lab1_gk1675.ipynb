{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE-GY 9163 Lab1 Guandong Kou (gk1675)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description of this lab is re-organized in this [Google doc](https://docs.google.com/document/d/1wp9a-ns_xwh2_x5FIJFrzo0JQZdOY9Dwtg9LXW8pYVw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename 'part10' to 'validation_set' at `lemm_stop` directory\n",
    "```!mv part10 validation_set```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import text\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "import re\n",
    "import numpy as np\n",
    "from heapq import nlargest\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and test dataset\n",
    "data_path = 'data/lemm_stop/part*/'\n",
    "spam_files = glob(data_path + 'spms*[0-9]*.txt')\n",
    "ham_files = glob(data_path + '*[0-9]*msg[0-9]*.txt')\n",
    "testset_path = 'data/lemm_stop/testset/'\n",
    "ts_spam_files = glob(testset_path + 'spms*[0-9]*.txt')\n",
    "ts_ham_files = glob(testset_path + '*[0-9]*msg[0-9]*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_spam, n_ham = len(spam_files), len(ham_files)\n",
    "n_total = n_spam + n_ham\n",
    "p_spam, p_ham = n_spam / n_total, n_ham / n_total\n",
    "\n",
    "n_ts_ham, n_ts_spam = len(ts_ham_files), len(ts_spam_files)\n",
    "n_test = n_ts_ham + n_ts_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counter(files):\n",
    "    # get word counter from files\n",
    "    remove_set = set(punctuation) | set(text.ENGLISH_STOP_WORDS)\n",
    "    w_counter = Counter()\n",
    "    for file in files:\n",
    "        word_set = set()\n",
    "        with open(file) as f:\n",
    "            for line in f.readlines():\n",
    "                for word in word_tokenize(line):\n",
    "                    if word not in remove_set and re.match('[a-zA-Z]{2,}', word):\n",
    "                        word_set.add(word.lower())\n",
    "        for word in word_set:\n",
    "            w_counter[word] += 1\n",
    "    return w_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_word_counter = get_word_counter(ham_files)\n",
    "spam_word_counter = get_word_counter(spam_files)\n",
    "total_word_counter = ham_word_counter + spam_word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(p):\n",
    "    return -(p * log2(p) + (1-p) * log2(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_word(word):\n",
    "    return total_word_counter[word] / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_H_C_X(word):\n",
    "    p_word = total_word_counter[word] / n_total\n",
    "    p_ham_cond_word = (ham_word_counter[word] + 1) / (total_word_counter[word] + 2) # Laplacian smoothing is applied here\n",
    "    p_ham_cond_not_word = (n_ham - ham_word_counter[word] + 1) / (n_total - total_word_counter[word] + 2) #\n",
    "    return p_word * get_entropy(p_ham_cond_word) + (1-p_word) * get_entropy(p_ham_cond_not_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_C = get_entropy(p_spam)\n",
    "def get_IG(word):\n",
    "    H_C_X = get_H_C_X(word)\n",
    "    return H_C - H_C_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.2014815280269308, 'language'),\n",
       " (0.1670505008347254, 'remove'),\n",
       " (0.16449082041657143, 'free'),\n",
       " (0.14515234460220117, 'linguistic'),\n",
       " (0.14253899233783307, 'university'),\n",
       " (0.11749366725618493, 'money'),\n",
       " (0.09944312815537659, 'click'),\n",
       " (0.09135660326353257, 'market'),\n",
       " (0.08584499224521047, 'business'),\n",
       " (0.0804619418318766, 'today')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 words with largest information gain \n",
    "n_largest_IG = nlargest(10, [(get_IG(w), w) for w in total_word_counter])\n",
    "n_largest_IG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "- Bernoulli NB classifier with binary features\n",
    "- Multinomial NB with binary features\n",
    "- Multinomial NB with term frequency (TF) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = 1000\n",
    "n_features_max = 1000\n",
    "top_n_features_map = {\n",
    "    word: idx for idx, (_, word) in enumerate(\n",
    "        nlargest(n_features_max, [(get_IG(w), w) for w in total_word_counter]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_to_matrix(files, M):\n",
    "    for idx, file in enumerate(files):\n",
    "        with open(file) as f:\n",
    "            for line in f.readlines():\n",
    "                for word in word_tokenize(line):\n",
    "                    if word in top_n_features_map:\n",
    "                        M[idx][top_n_features_map[word]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr = np.array([0] * n_ham + [1] * n_spam)\n",
    "Xtr = np.zeros((n_total, n_features_max))\n",
    "count_words_to_matrix(ham_files + spam_files, Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "yts = np.array([0] * n_ts_ham + [1] * n_ts_spam)\n",
    "Xts = np.zeros((n_test, n_features_max))\n",
    "count_words_to_matrix(ts_ham_files + ts_spam_files, Xts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_by_n_feat(X, n):\n",
    "    return X[:, :n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9450171821305842 10\n",
      "0.9484536082474226 100\n",
      "0.9484536082474226 1000\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli NB classifier with binary features;\n",
    "model_Bernoulli_NB = BernoulliNB()\n",
    "for n_features in [10, 100, 1000]:\n",
    "    _Xtr = get_X_by_n_feat(Xtr, n_features)\n",
    "    Xtr_binary = np.array([[1 if v > 0 else 0 for v in row] for row in _Xtr])\n",
    "    model_Bernoulli_NB.fit(Xtr_binary, ytr)\n",
    "    _Xts = get_X_by_n_feat(Xts, n_features)\n",
    "    Xts_binary = np.array([[1 if v > 0 else 0 for v in row] for row in _Xts])\n",
    "    yts_hat = model_Bernoulli_NB.predict(Xts_binary)\n",
    "    accuracy = np.mean(yts_hat == yts)\n",
    "    print(accuracy, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9518900343642611 10\n",
      "0.9828178694158075 100\n",
      "0.9896907216494846 1000\n"
     ]
    }
   ],
   "source": [
    "# MulPnomial NB with binary features\n",
    "model_Multinomial_NB = MultinomialNB()\n",
    "for n_features in [10, 100, 1000]:\n",
    "    Xtr_binary = np.array([[1 if v > 0 else 0 for v in row] for row in get_X_by_n_feat(Xtr, n_features)])\n",
    "    model_Multinomial_NB.fit(Xtr_binary, ytr)\n",
    "    Xts_binary = np.array([[1 if v > 0 else 0 for v in row] for row in get_X_by_n_feat(Xts, n_features)])\n",
    "    yts_hat = model_Multinomial_NB.predict(Xts_binary)\n",
    "    accuracy = np.mean(yts_hat == yts)\n",
    "    print(accuracy, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9896907216494846 10\n",
      "0.9896907216494846 100\n",
      "0.9896907216494846 1000\n"
     ]
    }
   ],
   "source": [
    "# MulPnomial NB with term frequency (TF) features\n",
    "model_Multinomial_NB = MultinomialNB()\n",
    "for n_features in [10, 100, 1000]:\n",
    "    model_Multinomial_NB.fit(Xtr, ytr)\n",
    "    yts_hat = model_Multinomial_NB.predict(Xts)\n",
    "    accuracy = np.mean(yts_hat == yts)\n",
    "    print(accuracy, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
